{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b2432a",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "**K-Means is an unsupervised learning algorithm that partitions data into a fixed number of clusters by minimizing the distance between data points and cluster centers.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 1: Initialization  \n",
    "- **Randomly pick $k$ points from the dataset as initial cluster centroids.**\n",
    "- **These centroids serve as the starting reference for each cluster.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 2: Cluster Assignment  \n",
    "- **Each point is assigned to the cluster whose centroid is closest (typically using Euclidean distance).**  \n",
    "- **We denote the index of the closest centroid to a point $x^{(i)}$ as $c^{(i)}$.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 3: Centroid Update  \n",
    "- **For each cluster, we recompute its centroid by taking the average of all data points assigned to it.**\n",
    "- **This new mean becomes the updated center for the cluster.**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Optimization Objective  \n",
    "\n",
    "**The algorithm minimizes the total within-cluster variance, or distortion cost, given by:**\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{m} \\left\\| x^{(i)} - \\mu_{c^{(i)}} \\right\\|^2\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $m$ is the total number of data points,  \n",
    "- $x^{(i)}$ is the $i$-th data point,  \n",
    "- $\\mu_{c^{(i)}}$ is the centroid of the cluster assigned to $x^{(i)}$.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Iteration & Convergence  \n",
    "- **Steps 2 and 3 are repeated until convergence, either when centroids stabilize or a predefined iteration limit is reached.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61c26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDecisionTree\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from mlfs.decision_tree import DecisionTree as CustomDecisionTree\n",
    "from mlfs.metrics import accuracy as custom_accuracy\n",
    "from mlfs.metrics import balanced_accuracy as custom_balanced_accuracy\n",
    "from mlfs.preprocessing import train_test_split, standardize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12b6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/iris.csv\")\n",
    "df.drop('Id',inplace=True,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d34e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].to_numpy() \n",
    "y = df.iloc[:, -1].to_numpy()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc870ed7",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#00EFFF;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '\n",
    "\n",
    "# Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc706440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kmeans_plot(X, n_clusters=3, max_iter=10):\n",
    "    \"\"\"\n",
    "    Compare clustering results of custom and sklearn KMeans using Plotly subplots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (2D, for visualization).\n",
    "    n_clusters : int\n",
    "        Number of clusters to form.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations for both models.\n",
    "    \"\"\"\n",
    "\n",
    "    custom_model = CustomKMeans(n_clusters=n_clusters, iterations=max_iter)\n",
    "    centroids_custom, labels_custom = custom_model.fit(X)\n",
    "\n",
    "    sklearn_model = SklearnKMeans(n_clusters=n_clusters, n_init=10, max_iter=max_iter, random_state=42)\n",
    "    labels_sklearn = sklearn_model.fit_predict(X)\n",
    "    centroids_sklearn = sklearn_model.cluster_centers_\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Custom KMeans\", \"Sklearn KMeans\"])\n",
    "\n",
    "    for cid in np.unique(labels_custom):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X[labels_custom == cid, 0], y=X[labels_custom == cid, 1],\n",
    "            mode='markers', name=f'Custom Cluster {cid}', legendgroup=f'c{cid}'\n",
    "        ), row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=centroids_custom[:, 0], y=centroids_custom[:, 1],\n",
    "        mode='markers', marker=dict(symbol='x', size=12, color='lime'),\n",
    "        name='Custom Centroids', legendgroup='centroids'\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    for cid in np.unique(labels_sklearn):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X[labels_sklearn == cid, 0], y=X[labels_sklearn == cid, 1],\n",
    "            mode='markers', name=f'Sklearn Cluster {cid}', legendgroup=f's{cid}', showlegend=False\n",
    "        ), row=1, col=2)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=centroids_sklearn[:, 0], y=centroids_sklearn[:, 1],\n",
    "        mode='markers', marker=dict(symbol='x', size=12, color='lightblue'),\n",
    "        name='Sklearn Centroids', legendgroup='centroids', showlegend=False\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_dark\",\n",
    "        width=1000,\n",
    "        height=500,\n",
    "        title=\"Comparison of Custom vs Sklearn KMeans\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "394b5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_kmeans_plot(X, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c84baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=0)\n",
    "compare_kmeans_plot(X, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7fd5f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].to_numpy() \n",
    "y = df.iloc[:, -1].to_numpy()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8f5a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kmeans_time_from_data(X, n_clusters=3, n_init=10, max_iter=300):\n",
    "    \"\"\"\n",
    "    Compares training and prediction time of custom vs sklearn KMeans.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Custom KMeans\n",
    "    custom_model = CustomKMeans(n_clusters=n_clusters, iterations=max_iter)\n",
    "    start = time.time()\n",
    "    _, _ = custom_model.fit(X)\n",
    "    fit_custom = time.time() - start\n",
    "\n",
    "    start = time.time()\n",
    "    _ = custom_model.predict(X)\n",
    "    predict_custom = time.time() - start\n",
    "\n",
    "    # Sklearn KMeans\n",
    "    sklearn_model = SklearnKMeans(n_clusters=n_clusters, n_init=n_init, max_iter=max_iter, random_state=42)\n",
    "    start = time.time()\n",
    "    sklearn_model.fit(X)\n",
    "    fit_sklearn = time.time() - start\n",
    "\n",
    "    start = time.time()\n",
    "    _ = sklearn_model.predict(X)\n",
    "    predict_sklearn = time.time() - start\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Model\": [\"CustomKMeans\", \"SklearnKMeans\"],\n",
    "        \"FitTime (s)\": [fit_custom, fit_sklearn],\n",
    "        \"PredictTime (s)\": [predict_custom, predict_sklearn]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd4a3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_df = compare_kmeans_time_from_data(X=X, n_clusters=3)\n",
    "print(\"\\n‚è±Ô∏è Por√≥wnanie czasu:\")\n",
    "print(timing_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c91b5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_kmeans_scalability(sample_sizes, n_clusters=3, n_features=4, max_iter=100):\n",
    "    \"\"\"\n",
    "    Benchmarks training time of CustomKMeans vs SklearnKMeans for increasing sample sizes.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for n_samples in sample_sizes:\n",
    "        X, _ = make_blobs(n_samples=n_samples, centers=n_clusters, n_features=n_features, random_state=42)\n",
    "\n",
    "        \n",
    "        custom_model = CustomKMeans(n_clusters=n_clusters, iterations=max_iter)\n",
    "        start = time.time()\n",
    "        custom_model.fit(X)\n",
    "        fit_custom = time.time() - start\n",
    "\n",
    "        \n",
    "        sklearn_model = SklearnKMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=10, random_state=42)\n",
    "        start = time.time()\n",
    "        sklearn_model.fit(X)\n",
    "        fit_sklearn = time.time() - start\n",
    "\n",
    "        records.append({\n",
    "            \"Samples\": n_samples,\n",
    "            \"CustomKMeans\": fit_custom,\n",
    "            \"SklearnKMeans\": fit_sklearn\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    " \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(df[\"Samples\"], df[\"CustomKMeans\"], marker=\"o\", label=\"CustomKMeans\")\n",
    "    plt.plot(df[\"Samples\"], df[\"SklearnKMeans\"], marker=\"o\", label=\"SklearnKMeans\")\n",
    "    plt.title(\"KMeans Training Time vs Sample Size\")\n",
    "    plt.xlabel(\"Number of Samples\")\n",
    "    plt.ylabel(\"Fit Time (seconds)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bb5a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [100, 500, 1000, 2000, 5000, 10000]\n",
    "scalability_df = benchmark_kmeans_scalability(sample_sizes, n_clusters=3, n_features=X.shape[1])\n",
    "print(\"\\nüìà Skalowalno≈õƒá:\")\n",
    "print(scalability_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2e8df",
   "metadata": {},
   "source": [
    "## üîç Comparison Summary\n",
    "\n",
    "- ‚öôÔ∏è **Speed:**  \n",
    "  `scikit-learn` KMeans was significantly faster than the custom implementation.  \n",
    "  **Reason:** It is implemented in optimized low-level code (Cython/C), using efficient memory access and vectorized NumPy operations.  \n",
    "  In contrast, the custom version relies on pure Python loops, which are inherently slower for large datasets.\n",
    "\n",
    "- üìà **Cluster Assignments:**  \n",
    "  Both implementations produced very similar cluster assignments and centroids across multiple datasets.  \n",
    "  **Reason:** Given the same initialization and convergence criteria, both aim to minimize the same distortion objective.\n",
    "\n",
    "- üßÆ **Inertia and Labels:**  \n",
    "  Final inertia values and label distributions were nearly identical between the two models.  \n",
    "  **Reason:** Despite implementation differences, the iterative optimization targets the same loss function:  \n",
    "  $$\n",
    "  J = \\sum_{i=1}^{m} \\left\\| x^{(i)} - \\mu_{c^{(i)}} \\right\\|^2\n",
    "  $$\n",
    "\n",
    "> ‚úÖ In summary: `scikit-learn` is significantly more efficient, but the custom implementation behaves correctly and yields matching results ‚Äî making it a reliable educational benchmark.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv mlfs)",
   "language": "python",
   "name": "mlfs_venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
