{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e31083",
   "metadata": {},
   "source": [
    "# Understanding Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes is a simple yet powerful classification technique based on **Bayes' Theorem**, which relates the conditional and marginal probabilities of random events.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Bayes' Theorem\n",
    "\n",
    "The fundamental equation:\n",
    "\n",
    "$$\n",
    "P(y \\mid X) = \\frac{P(X \\mid y) \\cdot P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P(y \\mid X) \\) is the **posterior**: the probability of class `y` given the features `X`.\n",
    "- \\( P(X \\mid y) \\) is the **likelihood**: the probability of the features `X` given that class `y` is true.\n",
    "- \\( P(y) \\) is the **prior**: the initial probability of class `y`.\n",
    "- \\( P(X) \\) is the **evidence**: the overall probability of the features `X` (constant for all classes).\n",
    "\n",
    "In practice, since \\( P(X) \\) is the same for every class, we often omit it when comparing probabilities between classes.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤” Why Is It Called *Naive*?\n",
    "\n",
    "The \"naive\" part comes from the **assumption of independence**: Naive Bayes assumes that all features are independent given the class. This is rarely true in real-world data, but the method still performs well.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Classification Objective\n",
    "\n",
    "Our goal is to find the class \\( y \\) that **maximizes the posterior**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y \\; P(y \\mid X)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y \\; P(X \\mid y) \\cdot P(y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Gaussian Naive Bayes\n",
    "\n",
    "If the features are continuous, we often assume that they follow a **normal distribution** within each class. The likelihood \\( P(x_i \\mid y) \\) is then computed using the **Gaussian probability density function**:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\mu \\) is the mean of the feature values for class `y`\n",
    "- \\( \\sigma \\) is the standard deviation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Log Probability Trick\n",
    "\n",
    "To avoid numerical underflow when multiplying small probabilities, we sum the **logarithms** of probabilities instead:\n",
    "\n",
    "$$\n",
    "\\log P(X \\mid y) = \\sum_{i=1}^{n} \\log P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "Thus, the final decision rule becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y \\; \\left( \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i \\mid y) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ›  Implementation Summary\n",
    "\n",
    "To implement Naive Bayes with Gaussian assumption:\n",
    "- Estimate \\( \\mu \\), \\( \\sigma \\), and \\( P(y) \\) for each class.\n",
    "- For each test example, compute log-likelihoods for all classes.\n",
    "- Predict the class with the highest posterior probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddac9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB as SklearnNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from mlfs.naive_bayes import NaiveBayes as CustomNB\n",
    "from mlfs.preprocessing import train_test_split, standardize\n",
    "from mlfs.metrics import (\n",
    "    accuracy as custom_accuracy,\n",
    "    precision as custom_precision,\n",
    "    recall as custom_recall,\n",
    "    f1_score as custom_f1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/breast-cancer.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed11aa",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#00EFFF;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '\n",
    "\n",
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5714c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad084cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnosis'] = (df['diagnosis'] == 'M').astype(int)\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr, cmap='viridis_r',annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c85fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "notincluded_columns = abs(corr['diagnosis'])[abs(corr['diagnosis'] < 0.25)]\n",
    "notincluded_columns = notincluded_columns.index.tolist()\n",
    "for col in notincluded_columns:\n",
    "  df.drop(col, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9024825",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('diagnosis', axis = 1).values\n",
    "y = df['diagnosis']\n",
    "print('Shape of X', X.shape)\n",
    "print('Shape of y', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "X_train, mean, std = standardize(X_train, return_params=True)\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2f395",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#00EFFF;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '\n",
    "\n",
    "# Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_naive_bayes_custom_vs_sklearn(X, y, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Benchmarks training and prediction times for custom and sklearn Naive Bayes implementations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix.\n",
    "    y : np.ndarray\n",
    "        Target vector (binary: 0/1).\n",
    "    n_repeats : int\n",
    "        Number of repetitions for averaging time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with average fit and predict times for both models.\n",
    "    \"\"\"\n",
    "\n",
    "    custom_fit_times = []\n",
    "    custom_predict_times = []\n",
    "    sklearn_fit_times = []\n",
    "    sklearn_predict_times = []\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "        # Custom Naive Bayes\n",
    "        model_custom = CustomNB()\n",
    "\n",
    "        start = time.time()\n",
    "        model_custom.fit(X, y)\n",
    "        custom_fit_times.append(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        model_custom.predict(X)\n",
    "        custom_predict_times.append(time.time() - start)\n",
    "\n",
    "        # Sklearn Naive Bayes\n",
    "        model_sklearn = SklearnNB()\n",
    "\n",
    "        start = time.time()\n",
    "        model_sklearn.fit(X, y)\n",
    "        sklearn_fit_times.append(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        model_sklearn.predict(X)\n",
    "        sklearn_predict_times.append(time.time() - start)\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'Model': ['CustomNB', 'SklearnNB'],\n",
    "        'FitTime': [np.mean(custom_fit_times), np.mean(sklearn_fit_times)],\n",
    "        'PredictTime': [np.mean(custom_predict_times), np.mean(sklearn_predict_times)]\n",
    "    })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = benchmark_naive_bayes_custom_vs_sklearn(X_train, y_train)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_nb_scalability_vs_sklearn(sample_sizes, n_features=4):\n",
    "    \"\"\"\n",
    "    Benchmarks and compares fit/predict times of custom and sklearn Naive Bayes models \n",
    "    as dataset size increases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_sizes : list[int]\n",
    "        List of dataset sizes to evaluate.\n",
    "    n_features : int\n",
    "        Number of features per sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with timing results for plotting and analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for n_samples in sample_sizes:\n",
    "        X, y = make_classification(\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features,\n",
    "            n_informative=n_features,\n",
    "            n_redundant=0,\n",
    "            n_classes=2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Custom Naive Bayes\n",
    "        custom_model = CustomNB()\n",
    "\n",
    "        start = time.time()\n",
    "        custom_model.fit(X, y)\n",
    "        fit_custom = time.time() - start\n",
    "\n",
    "        start = time.time()\n",
    "        custom_model.predict(X)\n",
    "        predict_custom = time.time() - start\n",
    "\n",
    "        records.append({\n",
    "            \"Samples\": n_samples,\n",
    "            \"Model\": \"CustomNB\",\n",
    "            \"FitTime\": fit_custom,\n",
    "            \"PredictTime\": predict_custom\n",
    "        })\n",
    "\n",
    "        # Sklearn Naive Bayes\n",
    "        sklearn_model = SklearnNB()\n",
    "\n",
    "        start = time.time()\n",
    "        sklearn_model.fit(X, y)\n",
    "        fit_sklearn = time.time() - start\n",
    "\n",
    "        start = time.time()\n",
    "        sklearn_model.predict(X)\n",
    "        predict_sklearn = time.time() - start\n",
    "\n",
    "        records.append({\n",
    "            \"Samples\": n_samples,\n",
    "            \"Model\": \"SklearnNB\",\n",
    "            \"FitTime\": fit_sklearn,\n",
    "            \"PredictTime\": predict_sklearn\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    sns.lineplot(data=df, x=\"Samples\", y=\"FitTime\", hue=\"Model\", marker=\"o\", ax=axs[0])\n",
    "    axs[0].set_title(\"Fit Time vs Sample Size\")\n",
    "    axs[0].set_ylabel(\"Time (s)\")\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    sns.lineplot(data=df, x=\"Samples\", y=\"PredictTime\", hue=\"Model\", marker=\"o\", ax=axs[1])\n",
    "    axs[1].set_title(\"Predict Time vs Sample Size\")\n",
    "    axs[1].set_ylabel(\"Time (s)\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc019842",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [100, 500, 1000, 2000, 5000]\n",
    "benchmark_nb_scalability_vs_sklearn(sample_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_nb_accuracy_basic(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Compares CustomNB and SklearnNB using classification metrics:\n",
    "    - accuracy\n",
    "    - precision\n",
    "    - recall\n",
    "    - f1_score\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Comparison of all metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Custom Naive Bayes\n",
    "    custom_model = CustomNB()\n",
    "    custom_model.fit(X_train, y_train)\n",
    "    preds_custom = custom_model.predict(X_test)\n",
    "\n",
    "    acc_custom = custom_accuracy(y_test, preds_custom)\n",
    "    prec_custom = custom_precision(y_test, preds_custom)\n",
    "    rec_custom = custom_recall(y_test, preds_custom)\n",
    "    f1_custom = custom_f1(y_test, preds_custom)\n",
    "\n",
    "    # Sklearn Naive Bayes\n",
    "    sklearn_model = SklearnNB()\n",
    "    sklearn_model.fit(X_train, y_train)\n",
    "    preds_sklearn = sklearn_model.predict(X_test)\n",
    "\n",
    "    acc_sklearn = accuracy_score(y_test, preds_sklearn)\n",
    "    prec_sklearn = precision_score(y_test, preds_sklearn)\n",
    "    rec_sklearn = recall_score(y_test, preds_sklearn)\n",
    "    f1_sklearn = f1_score(y_test, preds_sklearn)\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        \"Model\": [\"CustomNB\", \"SklearnNB\"],\n",
    "        \"Accuracy\": [acc_custom, acc_sklearn],\n",
    "        \"Precision\": [prec_custom, prec_sklearn],\n",
    "        \"Recall\": [rec_custom, rec_sklearn],\n",
    "        \"F1 Score\": [f1_custom, f1_sklearn]\n",
    "    })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_nb_accuracy_basic(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211e855",
   "metadata": {},
   "source": [
    "## Analysis: Custom Naive Bayes vs Sklearn GaussianNB\n",
    "\n",
    "### 1. Performance (Fit and Prediction Time)\n",
    "\n",
    "The custom implementation of Naive Bayes achieves extremely fast training times across all tested dataset sizes. This is expected, since training in Gaussian Naive Bayes is reduced to computing class-wise means, variances, and prior probabilities â€” operations that are simple and efficient even without heavy optimization.\n",
    "\n",
    "However, prediction time is noticeably slower compared to sklearnâ€™s implementation, and the difference becomes more pronounced as the number of samples increases. This likely results from the use of plain Python loops in my implementation, while sklearn leverages optimized C extensions and vectorized routines under the hood.\n",
    "\n",
    "In short, training is equally lightweight in both models, but sklearn handles prediction much more efficiently due to better use of low-level performance techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Classification Metrics (Accuracy, Precision, Recall, F1)\n",
    "\n",
    "In terms of predictive performance, both models yield nearly identical results on all evaluated metrics. This confirms that the underlying probability calculations and class decision logic in my implementation are mathematically correct and consistent with sklearnâ€™s behavior.\n",
    "\n",
    "The close match in accuracy, precision, recall, and F1-score suggests that both models handle the data equally well from a statistical standpoint. Any differences are negligible and not statistically significant.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- The custom model achieves high accuracy and matches sklearn in terms of predictive quality.\n",
    "- Prediction is significantly slower in the custom version, especially on larger datasets.\n",
    "- This trade-off is expected: sklearn is highly optimized, while my implementation prioritizes readability and clarity over speed.\n",
    "\n",
    "To improve prediction time, I could consider:\n",
    "- Replacing sample-wise loops with vectorized NumPy operations,\n",
    "- Avoiding redundant calculations inside the prediction loop,\n",
    "- Precomputing reusable components during inference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv mlfs)",
   "language": "python",
   "name": "mlfs_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
