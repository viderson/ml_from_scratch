{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd6ebc5",
   "metadata": {},
   "source": [
    "# Linear Regression Overview\n",
    "\n",
    "Linear regression is a basic supervised learning technique for estimating a continuous target variable from one or more input features. Its hypothesis function is\n",
    "\n",
    "$$\n",
    "h_{w,b}(x) = w\\,x + b\n",
    "$$\n",
    "\n",
    "where  \n",
    "- \\(h_{w,b}(x)\\) is the predicted value for input \\(x\\),  \n",
    "- \\(w\\) is the weight (slope),  \n",
    "- \\(b\\) is the bias (intercept).\n",
    "\n",
    "## Training Objective\n",
    "\n",
    "We want to find \\((w, b)\\) that minimize the discrepancy between predictions and true labels.\n",
    "\n",
    "### Prediction\n",
    "\n",
    "For each example \\(x^{(i)}\\), the model predicts\n",
    "\n",
    "$$\n",
    "\\hat y^{(i)} = h_{w,b}\\bigl(x^{(i)}\\bigr) = w\\,x^{(i)} + b.\n",
    "$$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "We measure error using the Mean Squared Error:\n",
    "\n",
    "$$\n",
    "L(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\bigl(\\hat y^{(i)} - y^{(i)}\\bigr)^{2}\n",
    "$$\n",
    "\n",
    "where  \n",
    "- \\(m\\) is the number of training examples,  \n",
    "- \\(y^{(i)}\\) is the true target for the \\(i\\)-th sample.\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Compute partial derivatives of \\(L\\) with respect to each parameter:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}\n",
    "= \\frac{1}{m}\\sum_{i=1}^{m} \\bigl(\\hat y^{(i)} - y^{(i)}\\bigr)\\,x^{(i)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}\n",
    "= \\frac{1}{m}\\sum_{i=1}^{m} \\bigl(\\hat y^{(i)} - y^{(i)}\\bigr).\n",
    "$$\n",
    "\n",
    "## Parameter Updates (Gradient Descent)\n",
    "\n",
    "Update rules using learning rate \\(\\eta\\):\n",
    "\n",
    "$$\n",
    "w \\;\\leftarrow\\; w - \\eta \\,\\frac{\\partial L}{\\partial w},\n",
    "\\qquad\n",
    "b \\;\\leftarrow\\; b - \\eta \\,\\frac{\\partial L}{\\partial b}.\n",
    "$$\n",
    "\n",
    "By repeating prediction → loss → gradients → update, the model parameters converge to the best fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from mlfs.linear_regression import LinearRegression as CustomLinearRegression\n",
    "from mlfs.metrics import mse as custom_mse, rmse as custom_rmse, r2_score as custom_r2\n",
    "from mlfs.preprocessing import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681aec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples, n_features=10, noise=0.1, random_state=42):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X = rng.randn(n_samples, n_features)\n",
    "    true_coef = rng.randn(n_features)\n",
    "    y = X.dot(true_coef) + noise * rng.randn(n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f8bdefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(n_samples_list, n_features=10, n_repeats=3):\n",
    "    results = []\n",
    "    for n in n_samples_list:\n",
    "        X, y = generate_data(n, n_features)\n",
    "        custom_times = {'fit': [], 'predict': []}\n",
    "        for _ in range(n_repeats):\n",
    "            model = CustomLinearRegression()\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X, y, verbose = False)\n",
    "            t1 = time.perf_counter()\n",
    "            model.predict(X[:10])\n",
    "            t2 = time.perf_counter()\n",
    "            custom_times['fit'].append(t1 - t0)\n",
    "            custom_times['predict'].append(t2 - t1)\n",
    "        sklearn_times = {'fit': [], 'predict': []}\n",
    "        for _ in range(n_repeats):\n",
    "            model = SklearnLinearRegression()\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X, y)\n",
    "            t1 = time.perf_counter()\n",
    "            model.predict(X[:10])\n",
    "            t2 = time.perf_counter()\n",
    "            sklearn_times['fit'].append(t1 - t0)\n",
    "            sklearn_times['predict'].append(t2 - t1)\n",
    "        results.append({\n",
    "            'n_samples': n,\n",
    "            'custom_fit': np.mean(custom_times['fit']),\n",
    "            'custom_predict': np.mean(custom_times['predict']),\n",
    "            'sklearn_fit': np.mean(sklearn_times['fit']),\n",
    "            'sklearn_predict': np.mean(sklearn_times['predict']),\n",
    "        })\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cc5ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_list = [100, 1_000, 5_000, 10_000, 20_000]\n",
    "df = benchmark(n_samples_list)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cbd3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.loglog(df['n_samples'], df['custom_fit'], marker='o', label='custom fit')\n",
    "plt.loglog(df['n_samples'], df['sklearn_fit'], marker='o', label='sklearn fit')\n",
    "plt.loglog(df['n_samples'], df['custom_predict'], marker='x', linestyle='--', label='custom predict')\n",
    "plt.loglog(df['n_samples'], df['sklearn_predict'], marker='x', linestyle='--', label='sklearn predict')\n",
    "plt.xlabel('n_samples (log)')\n",
    "plt.ylabel('Time [s] (log)')\n",
    "plt.title('Benchmark training and prediction time')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a6ddf",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- **Training Time**  \n",
    "  - Both implementations scale roughly linearly in _n_ (O(n)), but scikit-learn is ~30–40× faster thanks to optimized C/BLAS routines.  \n",
    "- **Prediction Time**  \n",
    "  - Custom `predict` is ~5–10× faster for a fixed test set, since it uses a single NumPy matrix multiply without extra overhead.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e962075",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\bigl(y_i - \\hat y_i\\bigr)^{2}\n",
    "$$\n",
    "\n",
    "**What it measures:**  \n",
    "- The average of the squared differences between true values \\(y_i\\) and predictions \\(\\hat y_i\\).  \n",
    "- Emphasizes larger errors due to the squaring operation.\n",
    "\n",
    "**Key points:**  \n",
    "- Lower MSE implies predictions are, on average, closer to actual targets.  \n",
    "- Particularly sensitive to outliers, since large residuals contribute quadratically.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "\\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}}\n",
    "$$\n",
    "\n",
    "**What it measures:**  \n",
    "- The square root of the average squared error—restores original units of the target.  \n",
    "\n",
    "**Key points:**  \n",
    "- Easier to interpret than MSE because it’s in the same scale as \\(y\\).  \n",
    "- Still amplifies the impact of large deviations.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Coefficient of Determination (R²)\n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat y_i)^{2}}{\\sum_{i=1}^{n}(y_i - \\bar y)^{2}}\n",
    "$$\n",
    "\n",
    "where \\(\\bar y\\) is the mean of the true values.\n",
    "\n",
    "**What it measures:**  \n",
    "- The fraction of variance in the target that the model accounts for.  \n",
    "\n",
    "**Key points:**  \n",
    "- Values range from \\(-\\infty\\) (poor) up to 1 (perfect fit).  \n",
    "- A higher \\(R^2\\) suggests stronger explanatory power, but it doesn’t guarantee that individual predictions are accurate or that the model generalizes well.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10757f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(10000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "custom_model = CustomLinearRegression()\n",
    "custom_model.fit(X_train, y_train, plot_cost=True)\n",
    "sk_model = SklearnLinearRegression()\n",
    "sk_model = sk_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_custom = custom_model.predict(X_test)\n",
    "y_pred_sk     = sk_model.predict(X_test)\n",
    "\n",
    "mse_c = custom_mse(y_test,y_pred_custom)\n",
    "rmse_c = custom_rmse(y_test, y_pred_custom)\n",
    "r2_c = custom_r2(y_test, y_pred_custom)\n",
    "\n",
    "mse_s  = mean_squared_error(y_test, y_pred_sk)\n",
    "rmse_s = np.sqrt(mse_s) \n",
    "r2_s   = sklearn_r2(y_test, y_pred_sk)\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'model': ['custom', 'sklearn'],\n",
    "    'MSE':   [mse_c, mse_s],\n",
    "    'RMSE':  [rmse_c, rmse_s],\n",
    "    'R2':    [r2_c, r2_s]\n",
    "})\n",
    "\n",
    "display(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv mlfs)",
   "language": "python",
   "name": "mlfs_venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
