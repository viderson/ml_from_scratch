{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd6ebc5",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "Linear regression is a fundamental model in machine learning used for predicting a continuous output variable based on input features. The model function for linear regression is represented as:\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "In this equation, $f_{w,b}(x)$ represents the predicted output, $w$ is the weight parameter, $b$ is the bias parameter, and $x$ is the input feature.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "To train a linear regression model, we aim to find the best values for the parameters $(w, b)$ that best fit our dataset.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "The forward pass is a step where we compute the linear regression output for the input data $X$ using the current weights and biases. It's essentially applying our model to the input data.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The cost function is used to measure how well our model is performing. It quantifies the difference between the predicted values and the actual values in our dataset. The cost function is defined as:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Here, $J(w, b)$ is the cost, $m$ is the number of training examples, $x^{(i)}$ is the input data for the $i$-th example, $y^{(i)}$ is the actual output for the $i$-th example, and $w$ and $b$ are the weight and bias parameters, respectively.\n",
    "\n",
    "### Backward Pass (Gradient Computation)\n",
    "\n",
    "The backward pass computes the gradients of the cost function with respect to the weights and biases. These gradients are crucial for updating the model parameters during training. The gradient formulas are as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{w,b}(X^{(i)}) - y^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{w,b}(X^{(i)}) - y^{(i)})X^{(i)}\n",
    "$$\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The training process involves iteratively updating the weights and biases to minimize the cost function. This is typically done through an optimization algorithm like gradient descent. The update equations for parameters are:\n",
    "\n",
    "$$w \\leftarrow w - \\alpha \\frac{\\partial J}{\\partial w}$$\n",
    "\n",
    "$$b \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "Here, $\\alpha$ represents the learning rate, which controls the step size during parameter updates.\n",
    "\n",
    "By iteratively performing the forward pass, computing the cost, performing the backward pass, and updating the parameters, the model learns to make better predictions and fit the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c043f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlfs.linear_regression.py import LogisticRegression as CustomLinearRegression\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
