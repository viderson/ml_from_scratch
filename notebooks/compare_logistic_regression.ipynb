{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1152e1",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a classic binary classification method that models the probability of an input belonging to the positive class by passing a linear combination through a sigmoid:\n",
    "\n",
    "$$\n",
    "h_{\\mathbf{w},b}(\\mathbf{x}) = \\sigma\\bigl(\\mathbf{w}^\\top \\mathbf{x} + b\\bigr),\n",
    "\\quad\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "## Prediction Step\n",
    "\n",
    "1. Compute the linear score (logit):\n",
    "   $$\n",
    "   z = \\mathbf{w}^\\top \\mathbf{x} + b.\n",
    "   $$\n",
    "2. Convert to probability:\n",
    "   $$\n",
    "   \\hat y = \\sigma(z).\n",
    "   $$\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "We use the binary cross‑entropy to measure how well the predicted probabilities match the true labels \\(y^{(i)}\\in\\{0,1\\}\\):\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b)\n",
    "= -\\frac{1}{m}\\sum_{i=1}^m \\Bigl[\n",
    "y^{(i)}\\ln(\\hat y^{(i)}) \\;+\\;(1 - y^{(i)})\\ln\\bigl(1 - \\hat y^{(i)}\\bigr)\n",
    "\\Bigr].\n",
    "$$\n",
    "\n",
    "## Gradient Computation\n",
    "\n",
    "The gradients of \\(J\\) with respect to the parameters are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}}\n",
    "= \\frac{1}{m}\\sum_{i=1}^m \\bigl(\\hat y^{(i)} - y^{(i)}\\bigr)\\,\\mathbf{x}^{(i)},\n",
    "\\qquad\n",
    "\\frac{\\partial J}{\\partial b}\n",
    "= \\frac{1}{m}\\sum_{i=1}^m \\bigl(\\hat y^{(i)} - y^{(i)}\\bigr).\n",
    "$$\n",
    "\n",
    "## Parameter Update (Gradient Descent)\n",
    "\n",
    "With a learning rate \\(\\eta\\), update parameters as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta\\,\\frac{\\partial J}{\\partial \\mathbf{w}},\n",
    "\\qquad\n",
    "b \\leftarrow b - \\eta\\,\\frac{\\partial J}{\\partial b}.\n",
    "$$\n",
    "\n",
    "By iterating prediction → loss → gradient → update, the model converges to values of \\(\\mathbf{w}\\) and \\(b\\) that best separate the two classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba02f8",
   "metadata": {},
   "source": [
    "**Visualize the shape of the  Logistic function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "from mlfs.logistic_regression import LogisticRegression as CustomLogisticRegression\n",
    "from mlfs.preprocessing import train_test_split, standardize\n",
    "from mlfs.metrics import (\n",
    "    accuracy as custom_accuracy,\n",
    "    precision as custom_precision,\n",
    "    recall as custom_recall,\n",
    "    f1_score as custom_f1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf57ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-12, 12, 200)\n",
    "fig = px.line(x=z, y=sigmoid(z),title='Logistic Function',template=\"plotly_dark\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edce292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/breast-cancer.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d02a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='diagnosis',\n",
    "    color='diagnosis', \n",
    "    template=\"plotly_dark\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654cac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='area_mean',\n",
    "    color='diagnosis',\n",
    "    template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c141780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='radius_mean',\n",
    "    color='diagnosis',\n",
    "    template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44ae86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='perimeter_mean',\n",
    "    color='diagnosis',\n",
    "    template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a76f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='smoothness_mean',\n",
    "    color='diagnosis',\n",
    "    template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='texture_mean',\n",
    "    color='diagnosis',\n",
    "    template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a27163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='symmetry_worst',\n",
    "    color='diagnosis',\n",
    "    template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d17df023",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='concavity_worst',\n",
    "    color='diagnosis',\n",
    "    template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5676f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='fractal_dimension_worst',\n",
    "    color='diagnosis',\n",
    "    template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647ebc6",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#00EFFF;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '\n",
    "\n",
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf91e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a158d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnosis'] = (df['diagnosis'] == 'M').astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effdbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr, cmap='mako_r',annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1098a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_target = abs(corr[\"diagnosis\"])\n",
    "relevant_features = cor_target[cor_target>0.2]\n",
    "names = [index for index, value in relevant_features.items()]\n",
    "names.remove('diagnosis')\n",
    "X = df[names].values\n",
    "y = df['diagnosis'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c6b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "X_train, mean, std = standardize(X_train, return_params=True)\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7020255",
   "metadata": {},
   "source": [
    "**When working with classification models, it's essential to evaluate how well they perform. Below are four key metrics commonly used to assess classification performance:**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Accuracy\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- Accuracy represents the proportion of correctly predicted samples among all predictions.\n",
    "- It's a widely used and intuitive metric for evaluating model performance.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher accuracy generally indicates better overall performance.\n",
    "- However, in cases of class imbalance, accuracy can be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Precision\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- Precision shows how many of the positive predictions were actually correct.\n",
    "- It’s especially important when false positives are costly or problematic.\n",
    "\n",
    "**Interpretation:**\n",
    "- A high precision score means the model is rarely wrong when it predicts a positive.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Recall (Sensitivity)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- Recall measures how many actual positive cases were correctly identified.\n",
    "- It's crucial in situations where missing a positive instance is risky (e.g., in medical diagnoses).\n",
    "\n",
    "**Interpretation:**\n",
    "- High recall indicates the model successfully captures most positive cases.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. F1-Score\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- The F1-score is the harmonic mean of precision and recall.\n",
    "- It provides a single score that balances both false positives and false negatives.\n",
    "\n",
    "**Interpretation:**\n",
    "- A high F1-score suggests that the model performs well in both identifying and correctly predicting positive instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d6a8d",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#00EFFF;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '\n",
    "\n",
    "# Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac926c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomLogisticRegression()\n",
    "custom_model.fit(X_train, y_train, iterations= 1000,  plot_cost=False)\n",
    "\n",
    "sk_model = SklearnLogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "sk_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_custom = custom_model.predict(X_test)\n",
    "y_pred_sk     = sk_model.predict(X_test)\n",
    "\n",
    "acc_c  = custom_accuracy(y_test, y_pred_custom)\n",
    "prec_c = custom_precision(y_test, y_pred_custom)\n",
    "rec_c  = custom_recall(y_test, y_pred_custom)\n",
    "f1_c   = custom_f1(y_test, y_pred_custom)\n",
    "\n",
    "acc_s  = accuracy_score(y_test, y_pred_sk)\n",
    "prec_s = precision_score(y_test, y_pred_sk)\n",
    "rec_s  = recall_score(y_test, y_pred_sk)\n",
    "f1_s   = f1_score(y_test, y_pred_sk)\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'model':     ['custom',       'sklearn'],\n",
    "    'Accuracy':  [acc_c,          acc_s],\n",
    "    'Precision': [prec_c,         prec_s],\n",
    "    'Recall':    [rec_c,          rec_s],\n",
    "    'F1-score':  [f1_c,           f1_s]\n",
    "})\n",
    "\n",
    "display(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61866a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_logistic_custom_data(X, y, n_repeats=3, iterations=1000):\n",
    "    \"\"\"\n",
    "    Compares the fit() and predict() times of a custom logistic regression implementation\n",
    "    and sklearn's implementation on the given data X and y.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray or pd.DataFrame): Input features.\n",
    "        y (np.ndarray or pd.Series): Labels (0 or 1).\n",
    "        n_repeats (int): Number of repetitions for averaging the timing.\n",
    "        iterations (int): Number of training iterations for the custom model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing fit and predict times.\n",
    "    \"\"\"\n",
    "    custom_times = {'fit': [], 'predict': []}\n",
    "    sklearn_times = {'fit': [], 'predict': []}\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "        # Custom model\n",
    "        model = CustomLogisticRegression()\n",
    "        t0 = time.perf_counter()\n",
    "        model.fit(X, y, iterations=1000)\n",
    "        t1 = time.perf_counter()\n",
    "        model.predict(X)  \n",
    "        t2 = time.perf_counter()\n",
    "        custom_times['fit'].append(t1 - t0)\n",
    "        custom_times['predict'].append(t2 - t1)\n",
    "\n",
    "        # Sklearn model\n",
    "        model = SklearnLogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "        t0 = time.perf_counter()\n",
    "        model.fit(X, y)\n",
    "        t1 = time.perf_counter()\n",
    "        model.predict(X)  \n",
    "        t2 = time.perf_counter()\n",
    "        sklearn_times['fit'].append(t1 - t0)\n",
    "        sklearn_times['predict'].append(t2 - t1)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'custom_fit':      [np.mean(custom_times['fit'])],\n",
    "        'custom_predict':  [np.mean(custom_times['predict'])],\n",
    "        'sklearn_fit':     [np.mean(sklearn_times['fit'])],\n",
    "        'sklearn_predict': [np.mean(sklearn_times['predict'])]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b61b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = benchmark_logistic_custom_data(X_train, y_train, n_repeats=5)\n",
    "display(df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29039db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].bar(['Custom', 'Sklearn'], [df_times['custom_fit'][0], df_times['sklearn_fit'][0]],\n",
    "            color=['#1f77b4', '#ff7f0e'])\n",
    "axes[0].set_title('Fit Time Comparison')\n",
    "axes[0].set_ylabel('Time (seconds)')\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "axes[1].bar(['Custom', 'Sklearn'], [df_times['custom_predict'][0], df_times['sklearn_predict'][0]],\n",
    "            color=['#1f77b4', '#ff7f0e'])\n",
    "axes[1].set_title('Predict Time Comparison')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.suptitle('Benchmark: Custom vs Sklearn Logistic Regression', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_logistic(n_samples_list, n_features=10, n_repeats=3):\n",
    "    \"\"\"\n",
    "    For each number of samples in `n_samples_list`:\n",
    "      - generates binary classification data,\n",
    "      - trains CustomLogisticRegression and measures the time for fit() and predict(),\n",
    "      - trains Sklearn's LogisticRegression and measures the time for fit() and predict().\n",
    "    Returns a DataFrame with columns:\n",
    "      ['n_samples', 'custom_fit', 'custom_predict', 'sklearn_fit', 'sklearn_predict']\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for n in n_samples_list:\n",
    "        X, y = make_classification(\n",
    "            n_samples=n,\n",
    "            n_features=n_features,\n",
    "            n_informative=max(2, n_features // 2),\n",
    "            n_redundant=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        custom_times = {'fit': [], 'predict': []}\n",
    "        for _ in range(n_repeats):\n",
    "            model = CustomLogisticRegression()\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X, y, iterations = 1000)\n",
    "            t1 = time.perf_counter()\n",
    "            model.predict(X[:10])\n",
    "            t2 = time.perf_counter()\n",
    "            custom_times['fit'].append(t1 - t0)\n",
    "            custom_times['predict'].append(t2 - t1)\n",
    "        \n",
    "        sklearn_times = {'fit': [], 'predict': []}\n",
    "        for _ in range(n_repeats):\n",
    "            model = SklearnLogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X, y)\n",
    "            t1 = time.perf_counter()\n",
    "            model.predict(X[:10])\n",
    "            t2 = time.perf_counter()\n",
    "            sklearn_times['fit'].append(t1 - t0)\n",
    "            sklearn_times['predict'].append(t2 - t1)\n",
    "        \n",
    "        results.append({\n",
    "            'n_samples':       n,\n",
    "            'custom_fit':      np.mean(custom_times['fit']),\n",
    "            'custom_predict':  np.mean(custom_times['predict']),\n",
    "            'sklearn_fit':     np.mean(sklearn_times['fit']),\n",
    "            'sklearn_predict': np.mean(sklearn_times['predict']),\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1605de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_benchmark(df, sample_col, custom_fit_col, sklearn_fit_col, custom_pred_col, sklearn_pred_col):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(df[sample_col], df[custom_fit_col], label='Custom fit')\n",
    "    plt.plot(df[sample_col], df[sklearn_fit_col], label='Sklearn fit')\n",
    "    plt.xlabel('Number of samples')\n",
    "    plt.ylabel('Fit time (s)')\n",
    "    plt.title('Fit time comparison')\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(df[sample_col], df[custom_pred_col], label='Custom predict')\n",
    "    plt.plot(df[sample_col], df[sklearn_pred_col], label='Sklearn predict')\n",
    "    plt.xlabel('Number of samples')\n",
    "    plt.ylabel('Predict time (s)')\n",
    "    plt.title('Predict time comparison')\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e23aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [100, 1_000, 10_000, 100_000]\n",
    "df_times = benchmark_logistic(ns, n_features=20, n_repeats=5)\n",
    "\n",
    "plot_benchmark(df_times,\n",
    "               sample_col='n_samples',\n",
    "               custom_fit_col='custom_fit',\n",
    "               sklearn_fit_col='sklearn_fit',\n",
    "               custom_pred_col='custom_predict',\n",
    "               sklearn_pred_col='sklearn_predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02d754",
   "metadata": {},
   "source": [
    "## Benchmark Results: Custom vs Sklearn Logistic Regression\n",
    "\n",
    "The plots below compare the training and prediction times of a custom implementation of logistic regression with scikit-learn's `LogisticRegression`. The experiment was performed on datasets of increasing size (100, 1,000, 10,000, and 100,000 samples) using 20 features, with each measurement averaged over 5 runs.\n",
    "\n",
    "### Fit Time Comparison (Left Plot)\n",
    "\n",
    "- The custom model shows significantly higher training times compared to scikit-learn.\n",
    "- This is expected due to the use of a plain **batch gradient descent** algorithm in the custom implementation, with a relatively small learning rate (`0.0001`) and potentially a large number of iterations.\n",
    "- The model does include **early stopping based on gradient norm**, which helps avoid unnecessary iterations, but is still far less efficient than the **LBFGS solver** used by `sklearn`, which benefits from advanced optimization techniques and low-level performance tuning.\n",
    "- As dataset size increases, the gap between the two implementations becomes larger.\n",
    "\n",
    "### Predict Time Comparison (Right Plot)\n",
    "\n",
    "- The custom model has a slightly faster prediction time, but the absolute difference is negligible (on the order of microseconds).\n",
    "- This is because the custom `predict()` method uses a minimal implementation: just a dot product and a threshold (sigmoid + 0.5), with no input validation or type checks.\n",
    "- Scikit-learn's model includes additional overhead to ensure robustness and compatibility, which slightly increases prediction time.\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "- The custom implementation is ideal for **educational purposes** and understanding the mechanics of logistic regression.\n",
    "- However, for large-scale or production tasks, `scikit-learn` remains the preferred option due to its **optimization, speed, and reliability**.\n",
    "- Future improvements to the custom model could include adding **regularization**, **adaptive learning rates**, or using **Numba/Cython** to speed up computations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv mlfs)",
   "language": "python",
   "name": "mlfs_venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
